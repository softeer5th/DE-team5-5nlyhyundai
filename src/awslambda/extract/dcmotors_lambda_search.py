import requests
from bs4 import BeautifulSoup
import json
import urllib.parse
import time
from datetime import datetime, timezone, timedelta
import os

from common_utils import (
    get_db_connection,
    upsert_post_tracking_data,
)

# âœ… User-Agent ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Connection": "keep-alive"
}

# âœ… ê¸°ë³¸ URL ì„¤ì •
BASIC_URL = "https://gall.dcinside.com/board/lists/?id=car_new1&page={page_num}&s_type=search_subject_memo&s_keyword={query}"
DCINSIDE_URL = "https://gall.dcinside.com"

# âœ… ì§ì ‘ ì„¤ì •í•  ë³€ìˆ˜ë“¤
max_pages = 20  # ğŸ”¹ ê° search_posì—ì„œ ìµœëŒ€ ëª‡ ê°œì˜ í˜ì´ì§€ë¥¼ íƒìƒ‰í• ì§€ ì„¤ì •

# âœ… Lambda Handler í•¨ìˆ˜
def lambda_handler(event, context):
    # âœ… DB ì—°ê²°
    conn = get_db_connection()
    if conn is None:
        print("[ERROR] DB ì—°ê²° ì‹¤íŒ¨")
        raise Exception("500 - DB ì—°ê²° ì‹¤íŒ¨")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": "DB ì—°ê²° ì‹¤íŒ¨"})
        }

    # ì´ê²Œ í¬ë¡¤ë§í•œ ì‹œê°„.
    checked_at_str = event.get('checked_at')
    # ISO 8601 í˜•ì‹ â†’ UTC ê¸°ì¤€
    if checked_at_str:
        event_time = datetime.fromisoformat(checked_at_str)
    else:
        event_time = datetime.now(timezone.utc).replace(tzinfo=None)   # Fallback
    # UTC ì‹œê°„ì— 9ì‹œê°„ì„ ë”í•´ KST ì‹œê°„ìœ¼ë¡œ ë³€í™˜
    checked_at = event_time + timedelta(hours=9)  # UTC+9 (KST)
    # KST ì‹œê°„ ì¶œë ¥ (í˜•ì‹: â€˜YYYY-MM-DD HH:MM:SSâ€™)
    print("í•œêµ­ ì‹œê°„:", checked_at.strftime('%Y-%m-%d %H:%M:%S'))
    
    # ê²€ìƒ‰í•  í‚¤ì›Œë“œ
    keyword = event.get('keyword')
    post_links = []  # âœ… í¬ë¡¤ë§í•œ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸

    encoded_query = urllib.parse.quote(keyword)

    prev_page_post_ids = set()

    for page in range(1, max_pages + 1):
        print(f"ğŸ“„ í˜„ì¬ í˜ì´ì§€: {page}, ê²€ìƒ‰ì–´: {keyword}")

        # ğŸ”¹ ê²€ìƒ‰ í˜ì´ì§€ URL ìƒì„±
        url = BASIC_URL.format(page_num=page, query=encoded_query)

        # ğŸ”¹ ê²Œì‹œê¸€ ëª©ë¡ ìš”ì²­
        response = requests.get(url, headers=HEADERS)
        if response.status_code != 200:
            print(f"ğŸ”´ ìš”ì²­ ì‹¤íŒ¨ (Status Code: {response.status_code}) - {url}")
            continue

        soup = BeautifulSoup(response.content, "html.parser")

        # ğŸ”¹ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        articles = soup.select("tr.ub-content.us-post")

        if not articles:
            print(f"ğŸ”´ í˜ì´ì§€ {page}ì—ì„œ ê²Œì‹œê¸€ ì—†ìŒ")
            break

        current_page_post_ids = set()

        for article in articles:
            try:
                # âœ… ê²Œì‹œê¸€ ì œëª© ë° URL ê°€ì ¸ì˜¤ê¸°
                title_element = article.select_one("td.gall_tit.ub-word a:nth-child(1)")
                if not title_element:
                    continue

                title = title_element.text.strip()
                link = DCINSIDE_URL + title_element["href"]

                # âœ… ëŒ“ê¸€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                reply_num_span = article.select_one("td.gall_tit.ub-word a.reply_numbox span.reply_num")
                if reply_num_span:
                    comment_count = int(reply_num_span.text.strip().replace("[", "").replace("]", ""))
                else:
                    comment_count = 0

                post_id = article.get("data-no")
                current_page_post_ids.add(post_id)

                # âœ… ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
                view_count_element = article.select_one("td.gall_count")
                view_count = int(view_count_element.text.strip().replace("ì¡°íšŒ ", "").replace(",", "")) if view_count_element else 0

                # âœ… ì‘ì„±ì¼ ê°€ì ¸ì˜¤ê¸°
                created_at_element = article.select_one("td.gall_date")
                created_at = created_at_element.get("title") if created_at_element else "Unknown"
                created_at = datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")

                print(f"ì œëª©: {title} | ëŒ“ê¸€ ìˆ˜: {comment_count} | í‚¤ì›Œë“œ: {keyword}")

                # âœ… ë°ì´í„° ì €ì¥ (checked_at ì¶”ê°€)
                post_data = {
                    "title": title,
                    "url": link,
                    "comment_count": comment_count,
                    "status": "CHANGED",
                    "keyword": keyword,
                    "checked_at": checked_at,
                    "post_id": post_id,
                    "view": view_count,
                    "created_at": created_at
                }
                post_links.append(post_data)

                # âœ… ê°œë³„ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© DBì— ì €ì¥
                upsert_post_tracking_data(
                    conn=conn,
                    table_name="probe_dcmotors",
                    payload=post_data
                )

            except Exception as e:
                print(f"âŒ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        if prev_page_post_ids and prev_page_post_ids == current_page_post_ids:
            print(f"ğŸ”´ ë™ì¼í•œ í˜ì´ì§€ í¬ë¡¤ë§ ë°˜ë³µ ê°ì§€, í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        prev_page_post_ids = current_page_post_ids

        # ğŸ”¹ ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(1)

    # âœ… DB ì—°ê²° ì¢…ë£Œ
    conn.close()

    print(f"âœ… ê²Œì‹œê¸€ URL í¬ë¡¤ë§ ì™„ë£Œ! {len(post_links)}ê°œì˜ URLì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    # âœ… Lambdaì—ì„œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "statusCode": 200,
        "body": json.dumps({"message": "í¬ë¡¤ë§ ì™„ë£Œ", "total_posts": len(post_links)}, ensure_ascii=False)
    }