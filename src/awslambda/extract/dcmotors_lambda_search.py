import requests
from bs4 import BeautifulSoup
import json
import urllib.parse
import time
from datetime import datetime, timezone, timedelta
import os

from common_utils import (
    get_db_connection,
    upsert_post_tracking_data,
)

# âœ… User-Agent ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Connection": "keep-alive"
}

# âœ… ê¸°ë³¸ URL ì„¤ì •
BASIC_URL = "https://gall.dcinside.com/board/lists/?id=car_new1&page={page_num}&search_pos={search_pos}&s_type=search_subject_memo&s_keyword={query}"
DCINSIDE_URL = "https://gall.dcinside.com"

# âœ… ì§ì ‘ ì„¤ì •í•  ë³€ìˆ˜ë“¤
search_positions = [-9645863, -9635863, -9625863]  # ğŸ”¹ í¬ë¡¤ë§í•  ê²€ìƒ‰ í¬ì§€ì…˜ ë¦¬ìŠ¤íŠ¸
max_pages = 15  # ğŸ”¹ ê° search_posì—ì„œ ìµœëŒ€ ëª‡ ê°œì˜ í˜ì´ì§€ë¥¼ íƒìƒ‰í• ì§€ ì„¤ì •
table_name = "probe_dcmotors"  # ğŸ”¹ ì‚¬ìš©í•  í…Œì´ë¸”
keyword_list = ["ë²¤ì¸ "]  # ğŸ”¹ ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸


# âœ… Lambda Handler í•¨ìˆ˜
def lambda_handler(event, context):
    # âœ… DB ì—°ê²°
    conn = get_db_connection()
    if conn is None:
        print("[ERROR] DB ì—°ê²° ì‹¤íŒ¨")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": "DB ì—°ê²° ì‹¤íŒ¨"})
        }
    
    # ì´ê²Œ í¬ë¡¤ë§í•œ ì‹œê°„.
    checked_at_str = event.get('checked_at')
    # ISO 8601 í˜•ì‹ â†’ UTC ê¸°ì¤€
    if checked_at_str:
        event_time = datetime.fromisoformat(checked_at_str.replace("Z", "+00:00"))
    else:  
        event_time = datetime.now(timezone.utc)  # Fallback
    # UTC ì‹œê°„ì— 9ì‹œê°„ì„ ë”í•´ KST ì‹œê°„ìœ¼ë¡œ ë³€í™˜
    checked_at = event_time + timedelta(hours=9)  # UTC+9 (KST)
    # KST ì‹œê°„ ì¶œë ¥ (í˜•ì‹: â€˜YYYY-MM-DD HH:MM:SSâ€™)
    print("í•œêµ­ ì‹œê°„:", checked_at.strftime('%Y-%m-%d %H:%M:%S'))

    # ê²Œì‹œê¸€ ì‹œì‘ ë‚ ì§œ
    start_date = event.get('start_date')
    if start_date is None:
        start_dt = checked_at - timedelta(days=14)
    else:
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')    
    
    # ê²Œì‹œê¸€ ì¢…ë£Œ ë‚ ì§œ
    end_date = event.get('end_date')
    if end_date is None:
        end_dt = checked_at + timedelta(days=1)
    else:
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    # ê²€ìƒ‰í•  í‚¤ì›Œë“œ
    keyword = event.get('keyword')
    post_links = []  # âœ… í¬ë¡¤ë§í•œ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸

    encoded_query = urllib.parse.quote(keyword)

    for search_pos in search_positions:
        for page in range(1, max_pages + 1):
            print(f"ğŸ“„ í˜„ì¬ í˜ì´ì§€: {page}, search_pos: {search_pos}, ê²€ìƒ‰ì–´: {keyword}")

            # ğŸ”¹ ê²€ìƒ‰ í˜ì´ì§€ URL ìƒì„±
            url = BASIC_URL.format(page_num=page, search_pos=search_pos, query=encoded_query)

            # ğŸ”¹ ê²Œì‹œê¸€ ëª©ë¡ ìš”ì²­
            response = requests.get(url, headers=HEADERS)
            if response.status_code != 200:
                print(f"ğŸ”´ ìš”ì²­ ì‹¤íŒ¨ (Status Code: {response.status_code}) - {url}")
                continue

            soup = BeautifulSoup(response.content, "html.parser")

            # ğŸ”¹ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            articles = soup.select("tr.ub-content.us-post")

            if not articles:
                print(f"ğŸ”´ {search_pos}ì˜ í˜ì´ì§€ {page}ì—ì„œ ê²Œì‹œê¸€ ì—†ìŒ")
                break

            for article in articles:
                try:
                    # âœ… ê²Œì‹œê¸€ ì œëª© ë° URL ê°€ì ¸ì˜¤ê¸°
                    title_element = article.select_one("td.gall_tit.ub-word a:nth-child(1)")
                    if not title_element:
                        continue

                    title = title_element.text.strip()
                    link = DCINSIDE_URL + title_element["href"]

                    # âœ… ëŒ“ê¸€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                    reply_num_span = article.select_one("td.gall_tit.ub-word a.reply_numbox span.reply_num")
                    if reply_num_span:
                        comment_count = int(reply_num_span.text.strip().replace("[", "").replace("]", ""))
                    else:
                        comment_count = 0

                    post_id = article.get("data-no")

                    # âœ… ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
                    view_count_element = article.select_one("td.gall_count")
                    view_count = int(view_count_element.text.strip().replace("ì¡°íšŒ ", "").replace(",", "")) if view_count_element else 0

                    # âœ… ì‘ì„±ì¼ ê°€ì ¸ì˜¤ê¸°
                    created_at_element = article.select_one("td.gall_date")
                    created_at = created_at_element.get("title") if created_at_element else "Unknown"
                    created_at = datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")

                    if created_at > end_dt:
                        print(f'[INFO] ê¸°ê°„ì´ ë” ë’¤ì´ê¸°ì— ë„˜ì–´ê°‘ë‹ˆë‹¤. {end_dt} / ê²Œì‹œê¸€ ë‚ ì§œ: {created_at}')
                        continue
                    
                    if created_at < start_dt:
                        print(f'[INFO] ê¸°ê°„ì´ ë” ì•ì´ê¸°ì— ì¢…ë£Œí•©ë‹ˆë‹¤. {start_dt} / ê²Œì‹œê¸€ ë‚ ì§œ: {created_at}')
                        return True

                    print(f"ì œëª©: {title} | ëŒ“ê¸€ ìˆ˜: {comment_count} | í‚¤ì›Œë“œ: {keyword}")

                    # âœ… ë°ì´í„° ì €ì¥ (checked_at ì¶”ê°€)
                    post_data = {
                        "title": title,
                        "url": link,
                        "comment_count": comment_count,
                        "status": "CHANGED",
                        "keyword": keyword,
                        "checked_at": checked_at,
                        "post_id": post_id,
                        "view": view_count,
                        "created_at": created_at
                    }
                    post_links.append(post_data)

                    # âœ… ê°œë³„ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© DBì— ì €ì¥
                    upsert_post_tracking_data(
                        conn=conn,
                        table_name=table_name,
                        payload=post_data
                    )

                except Exception as e:
                    print(f"âŒ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    continue

            # ğŸ”¹ ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)

    # âœ… DB ì—°ê²° ì¢…ë£Œ
    conn.close()

    print(f"âœ… ê²Œì‹œê¸€ URL í¬ë¡¤ë§ ì™„ë£Œ! {len(post_links)}ê°œì˜ URLì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    # âœ… Lambdaì—ì„œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "statusCode": 200,
        "body": json.dumps({"message": "í¬ë¡¤ë§ ì™„ë£Œ", "total_posts": len(post_links)}, ensure_ascii=False)
    }