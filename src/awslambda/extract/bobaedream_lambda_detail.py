import re
from typing import List, Dict, Optional, Union
from datetime import datetime, timezone, timedelta
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup

from bobaedream_utils import (
    clean_date_string,
    prepare_for_spark,
    save_html,
)

from common_utils import (
    get_db_connection,
    save_s3_bucket_by_parquet,
    upsert_post_tracking_data,
    get_details_to_parse,
    update_status_banned,
    update_status_failed,
    update_status_changed,
    update_status_unchanged,
    update_changed_stats,
    get_my_ip,
    analyze_post_with_gpt,
)

# ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ìœ„í•œ ì„¤ì •
analysis_executor = ThreadPoolExecutor(max_workers=20)

linebreak_ptrn = re.compile(r'(\n){2,}')  # ì¤„ë°”ê¿ˆ ë¬¸ì ë§¤ì¹­

def analyze_post(payload):
    """í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        print(f"ğŸ­ ê°ì„± ë¶„ì„ ì‹œì‘: {payload['url']}")
        analyzed_post = analyze_post_with_gpt(payload)
        print(f"âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ: {payload['url']}")
        return analyzed_post
    except Exception as e:
        print(f"âŒ ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        payload['sentiment'] = None
        for comment in payload['comment']:
            comment['sentiment'] = None
        return payload
    
def process_batch(futures: List) -> List[Dict]:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê°ì„± ë¶„ì„ ì²˜ë¦¬"""
    results = []
       
    # ì™„ë£Œëœ ì‘ì—…ë“¤ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
    for future in as_completed(futures):
        try:
            result = future.result()
            if result:
                results.append(result)
        except Exception as e:
            print(f"Error processing batch item: {e}")
    
    return results


def parse_post_meta(post, post_meta):
    # í¬ìŠ¤íŒ… ë©”íƒ€ë°ì´í„° ì¶”ê°€
    title_elem = post_meta.find('dt')
    if title_elem is None:
        print("[ERROR] ì œëª© íŒŒì‹± ì‹¤íŒ¨.")    
    post['title'] = title_elem['title']    
    count_group = post_meta.find('span', class_='countGroup')
    count_group_em = count_group.find_all('em')
    try:
        view = count_group_em[0].text
        view = int(view.replace(',', ''))  # ì‰¼í‘œ ì œê±°
    except Exception as e:
        print(f"[ERROR] ì¡°íšŒìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
        view = -999
    try:
        like = count_group_em[2].text
    except Exception as e:
        print(f"[ERROR] ì¢‹ì•„ìš” íŒŒì‹± ì‹¤íŒ¨: {e}")
        like = -999
    date_raw = count_group.text
    date_str = date_raw.split('|')[-1].strip()
    try:
        posting_datetime = clean_date_string(date_str)
    except Exception as e:
        print(f"[ERROR] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e}")
        posting_datetime = datetime.strptime('0000-01-01', '%Y-%m-%d')
    post['view'] = view
    post['like'] = like # like ë“±ì€ ì—¬ê¸°ì„œ ì²˜ë¦¬í•´ì•¼ í•¨.        
    post['dislike'] = None
    post['created_at'] = posting_datetime

def parse_detail() -> Union[List[Dict] | int | None]:
    """
    ê²€ìƒ‰ ê²°ê³¼ì˜ ê° ê²Œì‹œë¬¼ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ë¥¼ íŒŒì‹±í•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        search_results: ê²€ìƒ‰ëœ ê²Œì‹œë¬¼ ëª©ë¡ ([{title, link, ...}, ...])
    
    Returns:
        ìƒì„¸ ì •ë³´ê°€ ì¶”ê°€ëœ ê²Œì‹œë¬¼ ëª©ë¡
    """
    headers = {
        # 'User-Agent': UserAgent,
        'Host': 'www.bobaedream.co.kr',
        'Origin': 'https://www.bobaedream.co.kr',
        'Referer': 'https://www.bobaedream.co.kr/search',
        "Accept-Language":"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding":"gzip, deflate, br, zstd",
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'
        }
    
    conn = get_db_connection()
    if conn is None:
        print("[ERROR] DB ì—°ê²° ì‹¤íŒ¨")
        return 500
    
    # DBì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ê²Œì‹œë¬¼ ëª©ë¡
    table_name = 'probe_bobae'
    details = get_details_to_parse(conn, table_name)
    if details is None:
        print("[ERROR] DB ì¡°íšŒ ì‹¤íŒ¨")
        return 500
    
    if details == []:
        print("[INFO] íŒŒì‹±í•  ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 204
    
    payloads = []
    current_batch = []
    BATCH_SIZE = min(50, len(details))

    for post in details:
        try:
            post_url = post['url']
            print(f"ìš”ì²­ í”Œë«í¼: ë³´ë°°ë“œë¦¼ / '{post_url}' ê²€ìƒ‰ ì¤‘...")

            # ìƒì„¸ í˜ì´ì§€ ìš”ì²­
            time.sleep(1 + random.random())
            response = requests.get(
                    post_url,
                    headers=headers
                )
            response.encoding = 'utf-8'

            # cookies = response.cookies
            # response_headers = response.headers

            if response.status_code != 200:
                print(f"[ERROR] í™•ì¸ í•„ìš”! status code: {response.status_code}")
                if response.status_code == 403:
                    print(f"IP ì°¨ë‹¨ë¨! {response.status_code}")
                    update_status_banned(conn, table_name, post['url'])
                return 403, payloads
                    
            soup = BeautifulSoup(response.text, 'html.parser', from_encoding='utf-8')
            # ëŒ“ê¸€ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ì²˜ë¦¬
            has_comment = True
            try:
                comment_list = soup.find('div', id='cmt_list').find('ul', class_='basiclist').find_all('li')
                comment_count = len(comment_list)
            except Exception as e:
                print(f"[INFO] ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. {e} / post_url: {post_url}")
                has_comment = False
                comment_count = 0
            finally:
                post['comment_count'] = comment_count

            # ë³¸ë¬¸ ë‚´ìš© íŒŒì‹±
            content_element = soup.find('div', class_='bodyCont')
            try:
                cleaned_content = content_element.text.strip().replace('\xa0', ' ')
                cleaned_content = linebreak_ptrn.sub('\n', cleaned_content)
                post['content'] = cleaned_content
            except Exception as e:
                print(f"[ERROR] ë³¸ë¬¸ ë‚´ìš© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
            try:
                post_meta = soup.find('div', class_='writerProfile').find('dl')
            except Exception as e:
                print(f"[ERROR] í¬ìŠ¤íŒ… ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                post_meta = None
                continue
            if post_meta is None:
                print("[ERROR] í¬ìŠ¤íŒ… ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨.")
            else:
                # í¬ìŠ¤íŒ… ë©”íƒ€ë°ì´í„° íŒŒì‹±
                parse_post_meta(post, post_meta)
            
            # ëŒ“ê¸€ ì²˜ë¦¬
            comment_data = []
            if has_comment:
                for comment in comment_list: # 
                    if "ì‚­ì œëœ ëŒ“ê¸€ì…ë‹ˆë‹¤" in comment.text:
                        comment_data.append({
                            'created_at': None,
                            'content': None,
                            'like': None,
                            'dislike': None
                        })
                        continue
                    try:
                        comment_meta = comment.find('dl').find('dt').find_all('span')
                    except Exception as e:
                        print(f"[ERROR] ëŒ“ê¸€ ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                    
                    if comment_meta is None:
                        print("[ERROR] ëŒ“ê¸€ ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨.")
                        continue
                    try:
                        # comment_name = comment_meta[1].text
                        comment_date = datetime.strptime(comment_meta[3].text, '%y.%m.%d %H:%M')
                        comment_content = comment.find('dd').text.strip()
                        comment_like_dislike = comment.find('div', class_='updownbox').find_all('dd')
                        comment_like = comment_like_dislike[0].text.replace('ì¶”ì²œ ', '') 
                        comment_dislike = comment_like_dislike[1].text.replace('ë°˜ëŒ€ ', '')
                        comment_data.append({
                            'created_at': comment_date,
                            'content': comment_content,
                            'like': comment_like,
                            'dislike': comment_dislike
                        })
                    except Exception as e:
                        print(f"[ERROR] ëŒ“ê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            payload = {
                'checked_at': post['checked_at'],
                'platform': 'bobaedream',
                'title': post['title'],
                'post_id': post['post_id'],
                'url': post['url'],
                'content': post['content'],
                'view': post['view'],
                'created_at': post['created_at'],
                'like': post['like'],
                'dislike': post['dislike'],
                'comment_count': post['comment_count'],
                'keywords': post['keywords'],
                'comment': comment_data,
                'status': 'UNCHANGED',
            }       
            
            post['status'] = 'UNCHANGED'
            
            # ëª¨ë“  í¬ìŠ¤íŠ¸ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
            future = analysis_executor.submit(analyze_post, payload)
            current_batch.append(future)
            # ì™„ë£Œëœ ì‘ì—…ë“¤ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
            if len(current_batch) >= BATCH_SIZE:
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (í¬ê¸°: {len(current_batch)})")
                batch_results = process_batch(current_batch)
                payloads.extend(batch_results)
                current_batch = []

            is_success = update_changed_stats(conn, table_name, post['url'], post['comment_count'], post['view'], post['created_at'])            
            if is_success:
                print(f"[INFO] {post['url']} ì—…ë°ì´íŠ¸ ì„±ê³µ")
            else:
                print(f"[INFO] {post['url']} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            
        except Exception as e:
            post['status'] = 'FAILED'
            payloads.append({
                'status': 'FAILED',
            })
            update_status_failed(conn, table_name, post['url'])
            print(f"[ERROR] {post['url']} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ / ì´ìœ : {e}")    

    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if current_batch:
        print(f"ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ (í¬ê¸°: {len(current_batch)})")
        batch_results = process_batch(current_batch)
        payloads.extend(batch_results)
        
    return [payload for payload in payloads if payload['status'] == 'UNCHANGED']
        

def lambda_handler(event, context):
    # python -m bobaedream.bobaedream_exec ë¡œ ì‹¤í–‰
    get_my_ip()
    table_name = 'probe_bobae'
    details_data = parse_detail()
    if details_data == 500:
        return {
            "status_code": 500,
            "body": "[ERROR] DETAIL / DB ì—°ê²° ì‹¤íŒ¨"
        }
    if details_data == 204:
        return {
            "status_code": 204,
            "body": "[INFO] DETAIL / íŒŒì‹±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    elif details_data == []:
        return {
            "status_code": 201,
            "body": "[INFO] DETAIL / ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    try:
        checked_at_dt = details_data[0]['checked_at']
        save_s3_bucket_by_parquet(
            checked_at_dt=checked_at_dt,
            platform="bobaedream", 
            data=details_data
        )
        status_code = 200
        if isinstance(details_data, tuple) and details_data[0] == 403:
            status_code = details_data[0]
            details_data = details_data[1]
            print("[WARNING] DETAIL / ë³´ë°°ë“œë¦¼ IP ì°¨ë‹¨ë¨!")
        if status_code == 403:
            return {
                "status_code": 403,
                "body": f"[WARNING] DETAIL / IP ì°¨ë‹¨ë¨ / í¬ë¡¤ë§ ë°ì´í„°: {len(details_data)} ê±´"
            }
        return {
            "status_code": 200,
            "body": f"[INFO] S3 ì €ì¥ ì™„ë£Œ: {len(details_data)} ê±´"
        }
    except Exception as e:
        print(f"[ERROR] S3 ì €ì¥ ì‹¤íŒ¨: {e}")
        conn = get_db_connection()
        if details_data:
            for detail in details_data:
                update_status_failed(conn, table_name, detail["url"])
        return {
            "status_code": 500,
            "body": "[ERROR] S3 ì €ì¥ ì‹¤íŒ¨"
        }