import os
import json
import time
import queue
import threading
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from bs4 import BeautifulSoup
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from tempfile import mkdtemp
from selenium.webdriver.common.by import By
from common_utils import (
    get_db_connection, get_details_to_parse, upsert_post_tracking_data,
    save_s3_bucket_by_parquet, update_status_changed, update_changed_stats,
    analyze_post_with_gpt, update_status_failed
)

# ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ìœ„í•œ ì„¤ì •
analysis_executor = ThreadPoolExecutor(max_workers=20)

def setup_webdriver():
    """ì›¹ë“œë¼ì´ë²„ ì„¤ì • ë° ì‹¤í–‰"""
    chrome_options = ChromeOptions()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-setuid-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-dev-tools")
    chrome_options.add_argument("--no-zygote")
    chrome_options.add_argument("--single-process")
    chrome_options.add_argument(f"--user-data-dir={mkdtemp()}")
    chrome_options.add_argument(f"--data-path={mkdtemp()}")
    chrome_options.add_argument(f"--user-data-dir={mkdtemp()}")
    chrome_options.add_argument("--remote-debugging-pipe")
    chrome_options.add_argument("--verbose")
    chrome_options.add_argument("--log-path=/tmp")

    # Mac í™˜ê²½ íŠ¹í™” ì„¤ì • ì¶”ê°€
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--ignore-certificate-errors")
    chrome_options.add_argument("--disable-popup-blocking")

    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.managed_default_content_settings.ads": 2,
        "profile.managed_default_content_settings.media": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.plugins": 2
    }
    chrome_options.add_experimental_option("prefs", prefs)
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option("useAutomationExtension", False)

    try:
        chrome_options.binary_location = "/opt/chrome/chrome-linux64/chrome"
        service = Service("/opt/chrome-driver/chromedriver-linux64/chromedriver")
        driver = Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(30)
        return driver
    except Exception as e:
        print(f"âŒ ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        raise e

def crawl_post(driver, post, checked_at):
    """ì›¹ í¬ë¡¤ë§ ìˆ˜í–‰ í›„ ë°ì´í„°ë¥¼ ê°ì„± ë¶„ì„ìœ¼ë¡œ ë„˜ê¹€"""
    try:
        url = post["url"]
        print(f"ğŸ“ í¬ë¡¤ë§ ì‹œì‘: {url}")
        driver.get(url)
        time.sleep(1)  # í¬ë¡¤ë§ ê°„ê²© ì¡°ì •í•˜ì—¬ IP ì°¨ë‹¨ ë°©ì§€

        soup = BeautifulSoup(driver.page_source, "html.parser")

        title = soup.title.text.strip() if soup.title else "ì œëª© ì—†ìŒ"
        content = soup.select_one("div.write_div").text.strip() if soup.select_one("div.write_div") else "ë³¸ë¬¸ ì—†ìŒ"
        views = int(soup.select_one("span.gall_count").text.strip().replace("ì¡°íšŒ ", "")) if soup.select_one("span.gall_count") else post['view']
        likes = int(soup.select_one("div.up_num_box p.up_num").text.strip()) if soup.select_one("div.up_num_box p.up_num") else 0
        dislikes = int(soup.select_one("div.down_num_box p.down_num").text.strip()) if soup.select_one("div.down_num_box p.down_num") else 0
        comments_count = int(soup.select_one("span.gall_comment").text.strip().replace("ëŒ“ê¸€ ", "")) if soup.select_one("span.gall_comment") else post['comment_count']
        post_id = post.get("post_id", None)
        keywords = post.get("keywords", [])
        created_at = post["created_at"]

        # ğŸ”¹ ëŒ“ê¸€ í¬ë¡¤ë§
        def _get_post_comments():
            comment_elements = soup.select("li.ub-content div.clear.cmt_txtbox p.usertxt.ub-word")
            comments = []

            for el in comment_elements:
                comment_text = el.text.strip() if el else "ëŒ“ê¸€ ì—†ìŒ"
                comment_date = created_at  # ê¸°ë³¸ê°’

                comments.append({
                    "created_at": comment_date,
                    "content": comment_text,
                    "like": None,
                    "dislike": None
                })

            return comments

        comments = _get_post_comments()

        # ğŸ”¹ í¬ë¡¤ë§ ë°ì´í„° ì €ì¥ í›„ ì¦‰ì‹œ ê°ì„± ë¶„ì„ìœ¼ë¡œ ë„˜ê¹€
        temp_post = {
            "checked_at": checked_at,
            "platform": "dcinside",
            "title": title,
            "post_id": post_id,
            "url": url,
            "content": content,
            "view": views,
            "created_at": created_at,
            "like": likes,
            "dislike": dislikes,
            "comment_count": comments_count,
            "comment": comments,
            "keywords": keywords,
            "status": 'UNCHANGED'
        }
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ ë° ê°ì„± ë¶„ì„ ì‹œì‘: {url}")
        
        return temp_post

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

def analyze_post(post):
    """í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        print(f"ğŸ­ ê°ì„± ë¶„ì„ ì‹œì‘: {post['url']}")
        analyzed_post = analyze_post_with_gpt(post)
        print(f"âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ: {post['url']}")
        return analyzed_post
    
    except Exception as e:
        print(f"âŒ ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        post['sentiment'] = None
        for comment in post['comment']:
            comment['sentiment'] = None
        return post
    
def process_batch(futures: List) -> List[Dict]:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê°ì„± ë¶„ì„ ì²˜ë¦¬"""
    results = []
       
    # ì™„ë£Œëœ ì‘ì—…ë“¤ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
    for future in as_completed(futures):
        try:
            result = future.result()
            if result:
                results.append(result)
        except Exception as e:
            print(f"Error processing batch item: {e}")
    
    return results

def lambda_handler(event, context):
    id = event.get('id')
    checked_at = event.get('checked_at')
    checked_at = datetime.strptime(checked_at, "%Y-%m-%dT%H:%M:%S")

    start_time = time.time()
    """AWS Lambdaì—ì„œ ì‹¤í–‰ë˜ëŠ” í•¸ë“¤ëŸ¬ í•¨ìˆ˜"""
    driver = setup_webdriver()
    conn = get_db_connection()
    
    if conn is None:
        print("ğŸ”´ DB ì—°ê²° ì‹¤íŒ¨")
        raise "dc detail: 500 - DB ì—°ê²° ì‹¤íŒ¨"
        return {"status_code": 500, "body": "DB ì—°ê²° ì‹¤íŒ¨"}

    table_name = event.get("table_name", "probe_dcmotors")

    current_batch = []
    BATCH_SIZE = 50
    crawled_post = []

    # âœ… í¬ë¡¤ë§ â†’ ê°ì„± ë¶„ì„ ì¦‰ì‹œ ì‹¤í–‰ (ìˆœì°¨ ì²˜ë¦¬)
    while True:
        post = get_details_to_parse(conn, table_name)

        if post is None :
            raise "dc detail: 500 - DB ì—°ê²° ì‹¤íŒ¨"
            return {
            "status_code": 500,
            "body": "[ERROR] DETAIL / DB ì—°ê²° ì‹¤íŒ¨"
            }
        
        if post == [] :
            break

        temp_post = crawl_post(driver, post, checked_at)  # í¬ë¡¤ë§ê³¼ ë™ì‹œì— ê°ì„± ë¶„ì„ ìŠ¤ë ˆë“œ ì‹¤í–‰

        is_success = update_changed_stats(conn, table_name, post['url'], post['comment_count'], post['view'], post['created_at'])            
        if is_success:
            print(f"[INFO] {post['url']} ì—…ë°ì´íŠ¸ ì„±ê³µ")
        else:
            print(f"[INFO] {post['url']} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")

        # ëª¨ë“  í¬ìŠ¤íŠ¸ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
        future = analysis_executor.submit(analyze_post, temp_post)
        current_batch.append(future)

        if time.time() - start_time > 840 :
            print("14ë¶„ ê²½ê³¼")
            break

        # ì™„ë£Œëœ ì‘ì—…ë“¤ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
        if len(current_batch) >= BATCH_SIZE:
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (í¬ê¸°: {len(current_batch)})")
            batch_results = process_batch(current_batch)
            crawled_post.extend(batch_results)
            current_batch = []

    if current_batch:
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (í¬ê¸°: {len(current_batch)})")
        batch_results = process_batch(current_batch)
        crawled_post.extend(batch_results)
        current_batch = []

    try:
        save_s3_bucket_by_parquet(
            checked_at_dt=checked_at,
            platform="dcinside",
            data=list(crawled_post),
            id = id
        )
    except Exception as e:
        print(f"[ERROR] S3 ì €ì¥ ì‹¤íŒ¨: {e}")
        conn = get_db_connection()
        for failed_post in crawled_post:
            update_status_failed(conn, table_name, failed_post['url'])
        raise "dc detail: 500 - S3 ì €ì¥ ì‹¤íŒ¨"
        return {
            'status_code': 500,
            'body': '[ERROR] S3 ì €ì¥ ì‹¤íŒ¨'
        }

    driver.quit()
    return {"status_code": 200, "body": json.dumps({"body": "[INFO] DETAIL / S3 ì €ì¥ ì„±ê³µ"})}

