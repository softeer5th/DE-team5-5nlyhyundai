import os
import json
import time
from datetime import datetime
from bs4 import BeautifulSoup
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from tempfile import mkdtemp
from selenium.webdriver.common.by import By
from common_utils import get_db_connection, get_details_to_parse, upsert_post_tracking_data, save_s3_bucket_by_parquet, update_status_changed, update_changed_stats

def lambda_handler(event, context):
    # âœ… ì›¹ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •
    chrome_options = ChromeOptions()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-dev-tools")
    chrome_options.add_argument("--no-zygote")
    chrome_options.add_argument("--single-process")
    chrome_options.add_argument(f"--user-data-dir={mkdtemp()}")
    chrome_options.add_argument(f"--data-path={mkdtemp()}")
    chrome_options.add_argument(f"--disk-cache-dir={mkdtemp()}")
    chrome_options.add_argument("--remote-debugging-pipe")
    chrome_options.add_argument("--verbose")
    chrome_options.add_argument("--log-path=/tmp")
    chrome_options.binary_location = "/opt/chrome/chrome-linux64/chrome"
    prefs = {
        "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ë¹„í™œì„±í™”
        "profile.managed_default_content_settings.ads": 2,     # ê´‘ê³  ë¹„í™œì„±í™”
        "profile.managed_default_content_settings.media": 2    # ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ë¹„í™œì„±í™”
    }
    chrome_options.add_experimental_option("prefs", prefs)

    # âœ… Dockerì— ë¯¸ë¦¬ ì„¤ì¹˜ëœ Chromeê³¼ ChromeDriver ê²½ë¡œ ì„¤ì •
    chrome_options.binary_location = "/opt/chrome/chrome-linux64/chrome"
    service = Service("/opt/chrome-driver/chromedriver-linux64/chromedriver")

    # âœ… ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
    print("ğŸš€ ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰ ì¤‘...")
    driver = Chrome(service=service, options=chrome_options)

    # âœ… DB ì—°ê²°
    conn = get_db_connection()
    if conn is None:
        print("ğŸ”´ DB ì—°ê²° ì‹¤íŒ¨")
        return {"statusCode": 500, "body": "DB ì—°ê²° ì‹¤íŒ¨"}

    # âœ… í¬ë¡¤ë§í•  URL ê°€ì ¸ì˜¤ê¸°
    table_name = event.get("table_name", "probe_dcmotors")  # ê¸°ë³¸ í…Œì´ë¸” ì´ë¦„
    posts_to_crawl = get_details_to_parse(conn, table_name)

    if not posts_to_crawl:
        print("ğŸ”´ í¬ë¡¤ë§í•  ê²Œì‹œê¸€ ì—†ìŒ")
        driver.quit()
        return {"statusCode": 200, "body": "No posts to crawl"}

    print(f"ğŸ” í¬ë¡¤ë§í•  ê²Œì‹œê¸€ ìˆ˜: {len(posts_to_crawl)}")

    # âœ… í¬ë¡¤ë§í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    crawled_posts = []

    for idx, post in enumerate(posts_to_crawl):
        try:
            url = post["url"]
            print(f"ğŸ“ ({idx+1}/{len(posts_to_crawl)}) ê²Œì‹œê¸€ í¬ë¡¤ë§ ì‹œì‘: {url}")

            driver.get(url)
            time.sleep(1)  # â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

            soup = BeautifulSoup(driver.page_source, "html.parser")

            try:
                title = soup.title.text.strip()
            except:
                title = "ì œëª© ì—†ìŒ"

            try:
                content = soup.select_one("div.write_div").text.strip()
            except:
                content = "ë³¸ë¬¸ ì—†ìŒ"

            try:
                views = int(soup.select_one("span.gall_count").text.strip().replace("ì¡°íšŒ ", ""))
            except:
                views = post['view']

            try:
                likes = int(soup.select_one("div.up_num_box p.up_num").text.strip())
            except:
                likes = 0

            try:
                dislikes = int(soup.select_one("div.down_num_box p.down_num").text.strip())
            except:
                dislikes = 0

            try:
                comments_count = int(soup.select_one("span.gall_comment").text.strip().replace("ëŒ“ê¸€ ", ""))
            except:
                comments_count = post['commen_count']

            try:
                post_id = post['post_id']
            except:
                post_id = None

            created_at = post["created_at"].strftime("%Y-%m-%d %H:%M:%S")

            # ğŸ”¹ ëŒ“ê¸€ í¬ë¡¤ë§
            def _get_post_comments():
                comment_elements = soup.select("li.ub-content div.clear.cmt_txtbox p.usertxt.ub-word")
                comments = []

                for el in comment_elements:
                    try:
                        comment_text = el.text.strip()
                    except:
                        comment_text = "ëŒ“ê¸€ ì—†ìŒ"

                    try:
                        comment_date = el.find_parent("li").select_one("div.cmt_info span.date_time").text.strip()
                    except:
                        comment_date = created_at

                    comments.append({
                        "ì‘ì„±ì¼ì": comment_date,
                        "ë‚´ìš©": comment_text
                    })

                return comments

            comments = _get_post_comments()

            # ğŸ”¹ ë°ì´í„° ì €ì¥ì„ ìœ„í•´ Parquetìš© ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            crawled_posts.append({
                "platform": "DC",
                "title": title,
                "post_id": post_id,
                "url": url,
                "content": content,
                "view": views,
                "created_at": created_at,
                "like": likes,
                "dislike": dislikes,
                "comments_count": comments_count,
                "comment": comments
            })

            # ğŸ”¹ DBì—ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸
            update_result = update_changed_stats(conn, table_name, url, comments_count, views, created_at)

            if update_result:
                print(f"âœ… DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {url}")
            else:
                print(f"âŒ DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {url}")

        except Exception as e:
            print(f"âŒ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            continue

    # âœ… í¬ë¡¤ë§ ë°ì´í„° Parquet ì €ì¥ (S3 ì—…ë¡œë“œ)
    save_result = save_s3_bucket_by_parquet(
        start_dt=datetime.now(),
        platform="dcinside",
        data=crawled_posts
    )

    if save_result:
        print("âœ… Parquet íŒŒì¼ S3 ì €ì¥ ì™„ë£Œ")
    else:
        print("âŒ Parquet íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")

    # ğŸ”¹ ë¸Œë¼ìš°ì € ì¢…ë£Œ
    driver.quit()

    return {
        "statusCode": 200,
        "body": json.dumps({"message": "Crawling & S3 upload completed"}, ensure_ascii=False, indent=4)
    }
